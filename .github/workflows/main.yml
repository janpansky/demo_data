name: Daily Data Update

on:
  schedule:
    - cron: "0 2 * * *"  # Runs every day at 2:00 AM UTC
  workflow_dispatch:      # Allows manual triggering

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install polars-lts-cpu boto3

      - name: Generate Customers
        run: python scripts/generate_customers.py

      - name: Generate Orders
        run: python scripts/generate_orders.py

      - name: Generate Order Lines
        run: python scripts/generate_order_lines.py

      - name: Generate Returns
        run: python scripts/generate_returns.py

      - name: Generate Monthly Inventory
        run: python scripts/generate_monthly_inventory.py

      - name: Debug AWS key lengths
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "üïµÔ∏è AWS_ACCESS_KEY_ID length: ${#AWS_ACCESS_KEY_ID}"
          echo "üïµÔ∏è AWS_SECRET_ACCESS_KEY length: ${#AWS_SECRET_ACCESS_KEY}"

      - name: Upload CSVs to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        run: |
          python <<EOF
          import os, boto3
          data_dir = "data"
          s3 = boto3.client(
              "s3",
              aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
              aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
              region_name="us-east-1"  # Adjust if using a different region
          )
          bucket = os.getenv("AWS_S3_BUCKET")
          for fname in os.listdir(data_dir):
              if fname.endswith(".csv"):
                  file_path = os.path.join(data_dir, fname)
                  key = f"daily/{fname}"
                  print(f"‚è´ Uploading {file_path} to s3://{bucket}/{key}")
                  s3.upload_file(
                      Filename=file_path,
                      Bucket=bucket,
                      Key=key,
                      ExtraArgs={"ContentType": "text/csv"}
                  )
                  print(f"‚úÖ Uploaded {fname}")
          EOF